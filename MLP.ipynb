{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicoa1409031501/Niao/blob/main/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multilayer perceptron(MLP)範例\n",
        "- 基於惡意程式系統調用(api call)序列預測惡意程式家族"
      ],
      "metadata": {
        "id": "FF8bw64IqUrI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huRJne1kj68x",
        "outputId": "714ddead-3866-4e56-e7b6-ebc830ee0754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from random import shuffle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 載入原始資料集並將其切割成 trainset, validset, testset"
      ],
      "metadata": {
        "id": "VbsdM8leq6Eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(family2sample):\n",
        "    trainset = []\n",
        "    validset = []\n",
        "    testset = []\n",
        "\n",
        "    for family, samples in family2sample.items():\n",
        "        shuffle(samples)\n",
        "        trainset.extend(samples[:int(len(samples)*0.8)])\n",
        "        validset.extend(samples[int(len(samples)*0.8):int(len(samples)*0.9)])\n",
        "        testset.extend(samples[int(len(samples)*0.9):])\n",
        "    return trainset, validset, testset"
      ],
      "metadata": {
        "id": "68aRezACkJHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preload dataset\n",
        "## 請依據自己存放的檔案位置修改line3,line4的檔案位置\n",
        "trace_path = glob('/content/drive/MyDrive/demo_dataset/trace/*.trace')\n",
        "df_label = pd.read_csv('/content/drive/MyDrive/demo_dataset/label.csv')\n",
        "\n",
        "families = list(df_label['family'].unique())\n",
        "family2sample = {}\n",
        "for f in families:\n",
        "    df_family = df_label[df_label['family']==f].reset_index(drop = True)\n",
        "    family2sample[f] = df_family['sample'].tolist()\n",
        "sample2family = {row['sample']:row['family'] for index, row in df_label.iterrows()}\n",
        "\n",
        "# 將資料集切割成trainset, validset, testset\n",
        "trainset, validset, testset = split_dataset(family2sample)"
      ],
      "metadata": {
        "id": "VWt3zpO4kJwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "families"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKMeakqhRR86",
        "outputId": "a320f346-c862-4dc7-9e05-bc9568d515aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Emotet', 'Fareit', 'Gandcrab', 'Lokibot', 'Tofsee']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(trace_path[0]) as fp:\n",
        "  trace = [l.strip() for l in fp.readlines()]\n",
        "trace[:20]"
      ],
      "metadata": {
        "id": "p39JfTEjKaex",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb77c87e-6591-4611-80fd-13773ddaeb2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NtDelayExecution',\n",
              " 'NtAllocateVirtualMemory',\n",
              " 'RegOpenKeyExW',\n",
              " 'RegQueryValueExW',\n",
              " 'RegCloseKey',\n",
              " 'NtAllocateVirtualMemory',\n",
              " 'NtAllocateVirtualMemory',\n",
              " 'GetSystemWindowsDirectoryW',\n",
              " 'NtOpenFile',\n",
              " 'NtQueryInformationFile',\n",
              " 'NtClose',\n",
              " 'RegOpenKeyExW',\n",
              " 'RegQueryValueExW',\n",
              " 'RegCloseKey',\n",
              " 'RegOpenKeyExW',\n",
              " 'RegQueryValueExW',\n",
              " 'RegCloseKey',\n",
              " 'LdrGetDllHandle',\n",
              " 'LdrGetProcedureAddress',\n",
              " 'NtFreeVirtualMemory']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 資料集輸入特徵預處理"
      ],
      "metadata": {
        "id": "lO_A-kOirP56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 整理每筆資料的輸入\n",
        "sample2api = {}\n",
        "for p in tqdm(trace_path):\n",
        "  s = p.split('/')[-1].split('.trace')[0]\n",
        "  with open(p) as fp:\n",
        "    apis = [l.strip() for l in fp]\n",
        "  sample2api[s] = apis\n",
        "\n",
        "# 輸入特徵(input feature)預處理(One-Hot Encoding)\n",
        "all_api = []\n",
        "for s in tqdm(sample2api):\n",
        "  all_api.extend(sample2api[s])\n",
        "all_api = list(set(all_api))\n",
        "api_onehot = np.eye(len(all_api))\n",
        "\n",
        "idx2api = {i:a for i, a in enumerate(all_api)}\n",
        "api2idx = {a:i for i, a in enumerate(all_api)}\n",
        "idx2label = {i:f for i, f in enumerate(families)}\n",
        "label2idx = {f:i for i, f in enumerate(families)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IOxwUqrkKIa",
        "outputId": "ac3f1c16-55be-4fdb-a4a8-a1ba000394f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4126/4126 [01:57<00:00, 35.10it/s] \n",
            "100%|██████████| 4126/4126 [00:01<00:00, 2599.09it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 製作 torch dataset\n"
      ],
      "metadata": {
        "id": "UE3jRMhnsXey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_processed(dataset, label):\n",
        "    global api_onehot\n",
        "    global api2idx\n",
        "    global label2idx\n",
        "\n",
        "    padded_len = 100\n",
        "    processed_dataset = []\n",
        "    for s, apis in tqdm(dataset.items()):\n",
        "        input_apis = np.empty([0])\n",
        "        if len(apis) >= padded_len:\n",
        "            for a in apis[:padded_len]:\n",
        "                input_apis = np.concatenate((input_apis, api_onehot[api2idx[a]]))\n",
        "        else:\n",
        "            for a in apis:\n",
        "                input_apis = np.concatenate((input_apis, api_onehot[api2idx[a]]))\n",
        "            padding = np.zeros(244*(padded_len-len(apis)))\n",
        "            input_apis = np.concatenate((input_apis, padding))\n",
        "\n",
        "        processed_dataset.append({\n",
        "            'api': input_apis.astype('float32'),\n",
        "            'name': s,\n",
        "            'label': label2idx[label[s]]\n",
        "        })\n",
        "    return processed_dataset"
      ],
      "metadata": {
        "id": "fcVdbQw8kKeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_dataset = data_processed(sample2api, sample2family)\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = [], [], []\n",
        "for data in processed_dataset:\n",
        "    if data['name'] in trainset:\n",
        "        train_dataset.append(data)\n",
        "    elif data['name'] in validset:\n",
        "        valid_dataset.append(data)\n",
        "    elif data['name'] in testset:\n",
        "        test_dataset.append(data)\n",
        "\n",
        "# batch_size=64，一次取64筆資料進行訓練/驗證/測試\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = DataLoader(dataset=valid_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9x6TgLissaZ",
        "outputId": "671fc06a-aa9a-48f2-a211-716ed5f9773f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4126/4126 [00:03<00:00, 1220.24it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 訓練 Model"
      ],
      "metadata": {
        "id": "IuDpvq-iswC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 定義MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_size, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZD30te2-stFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 每筆資料的輸入長度固定為100個api call，每個api call向量維度為244\n",
        "padded_len = 100\n",
        "input_size = len(all_api) * padded_len # 244*100 = 24400\n",
        "output_size = len(families)\n",
        "\n",
        "model = MLP(input_size, output_size)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 30\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  train_losses = []\n",
        "  for i, data in enumerate(train_loader):\n",
        "    outputs = model(data['api'])\n",
        "    labels = data['label']\n",
        "    loss = loss_fn(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  valid_losses = []\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(valid_loader):\n",
        "      outputs = model(data['api'])\n",
        "      labels = data['label']\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "      total += labels.size(0)\n",
        "\n",
        "      valid_losses.append(loss.item())\n",
        "  print(f'epoch:{epoch} | train loss:{np.mean(train_losses):.4f} | valid loss:{np.mean(valid_losses):.4f} | valid acc:{100*correct/total:.3f}')"
      ],
      "metadata": {
        "id": "et2nZ7butAEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cde8d694-dd6c-4953-9da4-da03ca018c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0 | train loss:0.5044 | valid loss:0.2358 | valid acc:88.378\n",
            "epoch:1 | train loss:0.1735 | valid loss:0.2051 | valid acc:89.104\n",
            "epoch:2 | train loss:0.1417 | valid loss:0.2218 | valid acc:92.010\n",
            "epoch:3 | train loss:0.1299 | valid loss:0.2074 | valid acc:90.315\n",
            "epoch:4 | train loss:0.1251 | valid loss:0.2029 | valid acc:92.252\n",
            "epoch:5 | train loss:0.1186 | valid loss:0.2170 | valid acc:91.768\n",
            "epoch:6 | train loss:0.1208 | valid loss:0.1937 | valid acc:92.252\n",
            "epoch:7 | train loss:0.1203 | valid loss:0.2114 | valid acc:90.557\n",
            "epoch:8 | train loss:0.1153 | valid loss:0.2378 | valid acc:92.010\n",
            "epoch:9 | train loss:0.1144 | valid loss:0.2179 | valid acc:92.494\n",
            "epoch:10 | train loss:0.1143 | valid loss:0.2009 | valid acc:92.010\n",
            "epoch:11 | train loss:0.1142 | valid loss:0.2042 | valid acc:92.494\n",
            "epoch:12 | train loss:0.1177 | valid loss:0.2018 | valid acc:92.252\n",
            "epoch:13 | train loss:0.1131 | valid loss:0.2237 | valid acc:90.557\n",
            "epoch:14 | train loss:0.1128 | valid loss:0.2116 | valid acc:92.494\n",
            "epoch:15 | train loss:0.1129 | valid loss:0.2100 | valid acc:90.315\n",
            "epoch:16 | train loss:0.1111 | valid loss:0.2076 | valid acc:92.010\n",
            "epoch:17 | train loss:0.1114 | valid loss:0.2048 | valid acc:90.073\n",
            "epoch:18 | train loss:0.1106 | valid loss:0.2108 | valid acc:92.252\n",
            "epoch:19 | train loss:0.1098 | valid loss:0.2341 | valid acc:90.557\n",
            "epoch:20 | train loss:0.1084 | valid loss:0.2156 | valid acc:91.768\n",
            "epoch:21 | train loss:0.1105 | valid loss:0.2062 | valid acc:90.073\n",
            "epoch:22 | train loss:0.1098 | valid loss:0.2173 | valid acc:90.315\n",
            "epoch:23 | train loss:0.1090 | valid loss:0.2391 | valid acc:92.010\n",
            "epoch:24 | train loss:0.1096 | valid loss:0.2126 | valid acc:90.315\n",
            "epoch:25 | train loss:0.1107 | valid loss:0.2183 | valid acc:92.252\n",
            "epoch:26 | train loss:0.1107 | valid loss:0.2477 | valid acc:92.010\n",
            "epoch:27 | train loss:0.1091 | valid loss:0.2427 | valid acc:90.315\n",
            "epoch:28 | train loss:0.1103 | valid loss:0.2304 | valid acc:92.010\n",
            "epoch:29 | train loss:0.1069 | valid loss:0.2454 | valid acc:90.315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-WiIPZS_76JJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}